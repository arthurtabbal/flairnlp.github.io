"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[3302],{3905:(e,n,t)=>{t.d(n,{Zo:()=>m,kt:()=>c});var a=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function d(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function r(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var l=a.createContext({}),s=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):d(d({},n),e)),t},m=function(e){var n=s(e.components);return a.createElement(l.Provider,{value:n},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},g=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,m=r(e,["components","mdxType","originalType","parentName"]),p=s(t),g=i,c=p["".concat(l,".").concat(g)]||p[g]||u[g]||o;return t?a.createElement(c,d(d({ref:n},m),{},{components:t})):a.createElement(c,d({ref:n},m))}));function c(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var o=t.length,d=new Array(o);d[0]=g;var r={};for(var l in n)hasOwnProperty.call(n,l)&&(r[l]=n[l]);r.originalType=e,r[p]="string"==typeof e?e:i,d[1]=r;for(var s=2;s<o;s++)d[s]=t[s];return a.createElement.apply(null,d)}return a.createElement.apply(null,t)}g.displayName="MDXCreateElement"},3333:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>s});var a=t(7462),i=(t(7294),t(3905));const o={sidebar_position:5,description:"Other embeddings supported in Flair"},d="Other embeddings in Flair",r={unversionedId:"tutorial-embeddings/other-embeddings",id:"tutorial-embeddings/other-embeddings",title:"Other embeddings in Flair",description:"Other embeddings supported in Flair",source:"@site/docs/tutorial-embeddings/other-embeddings.md",sourceDirName:"tutorial-embeddings",slug:"/tutorial-embeddings/other-embeddings",permalink:"/docs/tutorial-embeddings/other-embeddings",draft:!1,editUrl:"https://github.com/flairNLP/flairnlp.github.io/edit/source/docs/tutorial-embeddings/other-embeddings.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5,description:"Other embeddings supported in Flair"},sidebar:"tutorialSidebar",previous:{title:"Classic word embeddings",permalink:"/docs/tutorial-embeddings/classic-word-embeddings"}},l={},s=[{value:"One-Hot Embeddings",id:"one-hot-embeddings",level:2},{value:"Vocabulary size",id:"vocabulary-size",level:3},{value:"Embedding dimensionality",id:"embedding-dimensionality",level:3},{value:"Embedding other tags",id:"embedding-other-tags",level:3},{value:"Byte Pair Embeddings",id:"byte-pair-embeddings",level:2},{value:"ELMo Embeddings",id:"elmo-embeddings",level:2},{value:"Document Pool Embeddings",id:"document-pool-embeddings",level:2},{value:"Pooling operation",id:"pooling-operation",level:3},{value:"Fine-tune mode",id:"fine-tune-mode",level:3},{value:"Document RNN Embeddings",id:"document-rnn-embeddings",level:2},{value:"RNN type",id:"rnn-type",level:3},{value:"Need to be trained on a task",id:"need-to-be-trained-on-a-task",level:3}],m={toc:s},p="wrapper";function u(e){let{components:n,...t}=e;return(0,i.kt)(p,(0,a.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"other-embeddings-in-flair"},"Other embeddings in Flair"),(0,i.kt)("p",null,"Flair supports many other embedding types. This section introduces these embeddings."),(0,i.kt)("admonition",{type:"info"},(0,i.kt)("p",{parentName:"admonition"},"We mostly train our models with either TransformerEmbeddings or FlairEmbeddings. The embeddings presented here might be useful\nfor specific use cases or for comparison purposes. ")),(0,i.kt)("h2",{id:"one-hot-embeddings"},"One-Hot Embeddings"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"OneHotEmbeddings")," are embeddings that encode each word in a vocabulary as a one-hot vector, followed by an embedding\nlayer. These embeddings\nthus do not encode any prior knowledge as do most other embeddings. They also differ in that they\nrequire to see a vocabulary (",(0,i.kt)("inlineCode",{parentName:"p"},"vocab_dictionary"),") during instantiation. Such dictionary can be passed as an argument\nduring class initialization or constructed directly from a corpus with a ",(0,i.kt)("inlineCode",{parentName:"p"},"from_corpus")," method. The dictionary consists\nof all unique tokens contained in the corpus plus an UNK token for all rare words."),(0,i.kt)("p",null,"You initialize these embeddings like this:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from flair.embeddings import OneHotEmbeddings\nfrom flair.datasets import UD_ENGLISH\nfrom flair.data import Sentence\n\n# load a corpus\ncorpus = UD_ENGLISH()\n\n# init embedding\nembeddings = OneHotEmbeddings.from_corpus(corpus)\n\n# create a sentence\nsentence = Sentence('The grass is green .')\n\n# embed words in sentence\nembeddings.embed(sentence)\n")),(0,i.kt)("p",null,"By default, the 'text' of a token (i.e. its lexical value) is one-hot encoded and the embedding layer has a dimensionality\nof 300. However, this layer is randomly initialized, meaning that these embeddings do not make sense unless they are trained in a task."),(0,i.kt)("h3",{id:"vocabulary-size"},"Vocabulary size"),(0,i.kt)("p",null,"By default, all words that occur in the corpus at least 3 times are part of the vocabulary. You can change\nthis using the ",(0,i.kt)("inlineCode",{parentName:"p"},"min_freq")," parameter. For instance, if your corpus is very large you might want to set a\nhigher ",(0,i.kt)("inlineCode",{parentName:"p"},"min_freq"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"embeddings = OneHotEmbeddings.from_corpus(corpus, min_freq=10)\n")),(0,i.kt)("h3",{id:"embedding-dimensionality"},"Embedding dimensionality"),(0,i.kt)("p",null,"By default, the embeddings have a dimensionality of 300. If you want to try higher or lower values, you can use the\n",(0,i.kt)("inlineCode",{parentName:"p"},"embedding_length")," parameter:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"embeddings = OneHotEmbeddings.from_corpus(corpus, embedding_length=100)\n")),(0,i.kt)("h3",{id:"embedding-other-tags"},"Embedding other tags"),(0,i.kt)("p",null,"Sometimes, you want to embed something other than text. For instance, sometimes we have part-of-speech tags or\nnamed entity annotation available that we might want to use. If this field exists in your corpus, you can embed\nit by passing the field variable. For instance, the UD corpora have a universal part-of-speech tag for each\ntoken ('upos'). Embed it like so:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from flair.datasets import UD_ENGLISH\nfrom flair.embeddings import OneHotEmbeddings\n\n# load corpus\ncorpus = UD_ENGLISH()\n\n# embed POS tags\nembeddings = OneHotEmbeddings.from_corpus(corpus, field='upos')\n")),(0,i.kt)("p",null,"This should print a vocabulary of size 18 consisting of universal part-of-speech tags."),(0,i.kt)("h2",{id:"byte-pair-embeddings"},"Byte Pair Embeddings"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"BytePairEmbeddings")," are word embeddings that are precomputed on the subword-level. This means that they are able to\nembed any word by splitting words into subwords and looking up their embeddings. ",(0,i.kt)("inlineCode",{parentName:"p"},"BytePairEmbeddings")," were proposed\nand computed by ",(0,i.kt)("a",{parentName:"p",href:"https://www.aclweb.org/anthology/L18-1473"},"Heinzerling and Strube (2018)")," who found that they offer nearly the same accuracy as word embeddings, but at a fraction\nof the model size. So they are a great choice if you want to train small models."),(0,i.kt)("p",null,"You initialize with a language code (275 languages supported), a number of 'syllables' (one of ) and\na number of dimensions (one of 50, 100, 200 or 300). The following initializes and uses byte pair embeddings\nfor English:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from flair.embeddings import BytePairEmbeddings\n\n# init embedding\nembedding = BytePairEmbeddings('en')\n\n# create a sentence\nsentence = Sentence('The grass is green .')\n\n# embed words in sentence\nembedding.embed(sentence)\n")),(0,i.kt)("p",null,"More information can be found\non the ",(0,i.kt)("a",{parentName:"p",href:"https://nlp.h-its.org/bpemb/"},"byte pair embeddings")," web page."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"BytePairEmbeddings")," also have a multilingual model capable of embedding any word in any language.\nYou can instantiate it with:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# init embedding\nembedding = BytePairEmbeddings('multi')\n")),(0,i.kt)("p",null,"You can also load custom ",(0,i.kt)("inlineCode",{parentName:"p"},"BytePairEmbeddings")," by specifying a path to model_file_path and embedding_file_path arguments. They correspond respectively to a SentencePiece model file and to an embedding file (Word2Vec plain text or GenSim binary). For example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# init custom embedding\nembedding = BytePairEmbeddings(model_file_path='your/path/m.model', embedding_file_path='your/path/w2v.txt')\n")),(0,i.kt)("h2",{id:"elmo-embeddings"},"ELMo Embeddings"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"http://www.aclweb.org/anthology/N18-1202"},"ELMo embeddings")," were presented by Peters et al. in 2018. They are using\na bidirectional recurrent neural network to predict the next word in a text.\nWe are using the implementation of ",(0,i.kt)("a",{parentName:"p",href:"https://allennlp.org/elmo"},"AllenNLP"),". As this implementation comes with a lot of\nsub-dependencies, which we don't want to include in Flair, you need to first install the library via\n",(0,i.kt)("inlineCode",{parentName:"p"},"pip install allennlp==0.9.0")," before you can use it in Flair.\nUsing the embeddings is as simple as using any other embedding type:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from flair.embeddings import ELMoEmbeddings\n\n# init embedding\nembedding = ELMoEmbeddings()\n\n# create a sentence\nsentence = Sentence('The grass is green .')\n\n# embed words in sentence\nembedding.embed(sentence)\n")),(0,i.kt)("p",null,"ELMo word embeddings can be constructed by combining ELMo layers in different ways. The available combination strategies are:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},'"all"'),": Use the concatenation of the three ELMo layers."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},'"top"'),": Use the top ELMo layer."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},'"average"'),": Use the average of the three ELMo layers.")),(0,i.kt)("p",null,"By default, the top 3 layers are concatenated to form the word embedding."),(0,i.kt)("p",null,"AllenNLP provides the following pre-trained models. To use any of the following models inside Flair\nsimple specify the embedding id when initializing the ",(0,i.kt)("inlineCode",{parentName:"p"},"ELMoEmbeddings"),"."),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"ID"),(0,i.kt)("th",{parentName:"tr",align:null},"Language"),(0,i.kt)("th",{parentName:"tr",align:null},"Embedding"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"'small'"),(0,i.kt)("td",{parentName:"tr",align:null},"English"),(0,i.kt)("td",{parentName:"tr",align:null},"1024-hidden, 1 layer, 14.6M parameters")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"'medium'"),(0,i.kt)("td",{parentName:"tr",align:null},"English"),(0,i.kt)("td",{parentName:"tr",align:null},"2048-hidden, 1 layer, 28.0M parameters")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"'original'"),(0,i.kt)("td",{parentName:"tr",align:null},"English"),(0,i.kt)("td",{parentName:"tr",align:null},"4096-hidden, 2 layers, 93.6M parameters")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"'large'"),(0,i.kt)("td",{parentName:"tr",align:null},"English"),(0,i.kt)("td",{parentName:"tr",align:null})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"'pt'"),(0,i.kt)("td",{parentName:"tr",align:null},"Portuguese"),(0,i.kt)("td",{parentName:"tr",align:null})),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"'pubmed'"),(0,i.kt)("td",{parentName:"tr",align:null},"English biomedical data"),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("a",{parentName:"td",href:"https://allennlp.org/elmo"},"more information"))))),(0,i.kt)("h2",{id:"document-pool-embeddings"},"Document Pool Embeddings"),(0,i.kt)("p",null,"DocumentPoolEmbeddings calculate a pooling operation over all word embeddings in a document.\nThe default operation is ",(0,i.kt)("inlineCode",{parentName:"p"},"mean")," which gives us the mean of all words in the sentence.\nThe resulting embedding is taken as document embedding."),(0,i.kt)("p",null,"To create a mean document embedding simply create any number of ",(0,i.kt)("inlineCode",{parentName:"p"},"TokenEmbeddings")," first and put them in a list.\nAfterwards, initiate the ",(0,i.kt)("inlineCode",{parentName:"p"},"DocumentPoolEmbeddings")," with this list of ",(0,i.kt)("inlineCode",{parentName:"p"},"TokenEmbeddings"),".\nSo, if you want to create a document embedding using GloVe embeddings together with ",(0,i.kt)("inlineCode",{parentName:"p"},"FlairEmbeddings"),",\nuse the following code:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n\n# initialize the word embeddings\nglove_embedding = WordEmbeddings('glove')\n\n# initialize the document embeddings, mode = mean\ndocument_embeddings = DocumentPoolEmbeddings([glove_embedding])\n")),(0,i.kt)("p",null,"Now, create an example sentence and call the embedding's ",(0,i.kt)("inlineCode",{parentName:"p"},"embed()")," method."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# create an example sentence\nsentence = Sentence('The grass is green . And the sky is blue .')\n\n# embed the sentence with our document embedding\ndocument_embeddings.embed(sentence)\n\n# now check out the embedded sentence.\nprint(sentence.embedding)\n")),(0,i.kt)("p",null,"This prints out the embedding of the document. Since the document embedding is derived from word embeddings, its dimensionality depends on the dimensionality of word embeddings you are using."),(0,i.kt)("p",null,"You have the following optional constructor arguments:"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Argument"),(0,i.kt)("th",{parentName:"tr",align:null},"Default"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"fine_tune_mode")),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"linear")),(0,i.kt)("td",{parentName:"tr",align:null},"One of ",(0,i.kt)("inlineCode",{parentName:"td"},"linear"),", ",(0,i.kt)("inlineCode",{parentName:"td"},"nonlinear")," and ",(0,i.kt)("inlineCode",{parentName:"td"},"none"),".")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"pooling")),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"first")),(0,i.kt)("td",{parentName:"tr",align:null},"One of ",(0,i.kt)("inlineCode",{parentName:"td"},"mean"),", ",(0,i.kt)("inlineCode",{parentName:"td"},"max")," and ",(0,i.kt)("inlineCode",{parentName:"td"},"min"),".")))),(0,i.kt)("h3",{id:"pooling-operation"},"Pooling operation"),(0,i.kt)("p",null,"Next to the ",(0,i.kt)("inlineCode",{parentName:"p"},"mean")," pooling operation you can also use ",(0,i.kt)("inlineCode",{parentName:"p"},"min")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"max")," pooling. Simply pass the pooling operation you want\nto use to the initialization of the ",(0,i.kt)("inlineCode",{parentName:"p"},"DocumentPoolEmbeddings"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"document_embeddings = DocumentPoolEmbeddings([glove_embedding],  pooling='min')\n")),(0,i.kt)("h3",{id:"fine-tune-mode"},"Fine-tune mode"),(0,i.kt)("p",null,"You can also choose which fine-tuning operation you want, i.e. which transformation to apply before word embeddings get\npooled. The default operation is 'linear' transformation, but if you only use simple word embeddings that are\nnot task-trained you should probably use a 'nonlinear' transformation instead:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# instantiate pre-trained word embeddings\nembeddings = WordEmbeddings('glove')\n\n# document pool embeddings\ndocument_embeddings = DocumentPoolEmbeddings([embeddings], fine_tune_mode='nonlinear')\n")),(0,i.kt)("p",null,"If on the other hand you use word embeddings that are task-trained (such as simple one hot encoded embeddings), you\nare often better off doing no transformation at all. Do this by passing 'none':"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# instantiate one-hot encoded word embeddings\nembeddings = OneHotEmbeddings(corpus)\n\n# document pool embeddings\ndocument_embeddings = DocumentPoolEmbeddings([embeddings], fine_tune_mode='none')\n")),(0,i.kt)("h2",{id:"document-rnn-embeddings"},"Document RNN Embeddings"),(0,i.kt)("p",null,"Besides simple pooling we also support a method based on an RNN to obtain a ",(0,i.kt)("inlineCode",{parentName:"p"},"DocumentEmbeddings"),".\nThe RNN takes the word embeddings of every token in the document as input and provides its last output state as document\nembedding. You can choose which type of RNN you wish to use."),(0,i.kt)("p",null,"In order to use the ",(0,i.kt)("inlineCode",{parentName:"p"},"DocumentRNNEmbeddings")," you need to initialize them by passing a list of token embeddings to it:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n\nglove_embedding = WordEmbeddings('glove')\n\ndocument_embeddings = DocumentRNNEmbeddings([glove_embedding])\n")),(0,i.kt)("p",null,"By default, a GRU-type RNN is instantiated. Now, create an example sentence and call the embedding's ",(0,i.kt)("inlineCode",{parentName:"p"},"embed()")," method."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# create an example sentence\nsentence = Sentence('The grass is green . And the sky is blue .')\n\n# embed the sentence with our document embedding\ndocument_embeddings.embed(sentence)\n\n# now check out the embedded sentence.\nprint(sentence.get_embedding())\n")),(0,i.kt)("p",null,"This will output a single embedding for the complete sentence. The embedding dimensionality depends on the number of\nhidden states you are using and whether the RNN is bidirectional or not."),(0,i.kt)("h3",{id:"rnn-type"},"RNN type"),(0,i.kt)("p",null,"If you want to use a different type of RNN, you need to set the ",(0,i.kt)("inlineCode",{parentName:"p"},"rnn_type")," parameter in the constructor. So,\nto initialize a document RNN embedding with an LSTM, do:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n\nglove_embedding = WordEmbeddings('glove')\n\ndocument_lstm_embeddings = DocumentRNNEmbeddings([glove_embedding], rnn_type='LSTM')\n")),(0,i.kt)("h3",{id:"need-to-be-trained-on-a-task"},"Need to be trained on a task"),(0,i.kt)("p",null,"Note that while ",(0,i.kt)("inlineCode",{parentName:"p"},"DocumentPoolEmbeddings")," are immediately meaningful, ",(0,i.kt)("inlineCode",{parentName:"p"},"DocumentRNNEmbeddings")," need to be tuned on the\ndownstream task. This happens automatically in Flair if you train a new model with these embeddings. "),(0,i.kt)("p",null,"Once the model is trained, you can access the tuned ",(0,i.kt)("inlineCode",{parentName:"p"},"DocumentRNNEmbeddings")," object directly from the classifier object and use it to embed sentences."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"document_embeddings = classifier.document_embeddings\n\nsentence = Sentence('The grass is green . And the sky is blue .')\n\ndocument_embeddings.embed(sentence)\n\nprint(sentence.get_embedding())\n")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"DocumentRNNEmbeddings")," have a number of hyper-parameters that can be tuned to improve learning:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-text"},":param hidden_size: the number of hidden states in the rnn.\n:param rnn_layers: the number of layers for the rnn.\n:param reproject_words: boolean value, indicating whether to reproject the token embeddings in a separate linear\nlayer before putting them into the rnn or not.\n:param reproject_words_dimension: output dimension of reprojecting token embeddings. If None the same output\ndimension as before will be taken.\n:param bidirectional: boolean value, indicating whether to use a bidirectional rnn or not.\n:param dropout: the dropout value to be used.\n:param word_dropout: the word dropout value to be used, if 0.0 word dropout is not used.\n:param locked_dropout: the locked dropout value to be used, if 0.0 locked dropout is not used.\n:param rnn_type: one of 'RNN' or 'LSTM'\n")))}u.isMDXComponent=!0}}]);