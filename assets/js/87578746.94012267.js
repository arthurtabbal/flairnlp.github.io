"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1638],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),d=p(n),h=i,m=d["".concat(s,".").concat(h)]||d[h]||u[h]||r;return n?a.createElement(m,l(l({ref:t},c),{},{components:n})):a.createElement(m,l({ref:t},c))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,l=new Array(r);l[0]=h;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[d]="string"==typeof e?e:i,l[1]=o;for(var p=2;p<r;p++)l[p]=n[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},8198:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>p});var a=n(7462),i=(n(7294),n(3905));const r={sidebar_position:2,description:"How model training works in Flair"},l="How model training works in Flair",o={unversionedId:"tutorial-training/how-model-training-works",id:"tutorial-training/how-model-training-works",title:"How model training works in Flair",description:"How model training works in Flair",source:"@site/docs/tutorial-training/how-model-training-works.md",sourceDirName:"tutorial-training",slug:"/tutorial-training/how-model-training-works",permalink:"/docs/tutorial-training/how-model-training-works",draft:!1,editUrl:"https://github.com/flairNLP/flairnlp.github.io/edit/source/docs/tutorial-training/how-model-training-works.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,description:"How model training works in Flair"},sidebar:"tutorialSidebar",previous:{title:"Tutorial 3: Training models",permalink:"/docs/category/tutorial-3-training-models"},next:{title:"How to load a prepared dataset",permalink:"/docs/tutorial-training/how-to-load-prepared-dataset"}},s={},p=[{value:"Example: Training a Part-of-Speech Tagger",id:"example-training-a-part-of-speech-tagger",level:2},{value:"Step-By-Step Walkthrough",id:"step-by-step-walkthrough",level:2},{value:"Step 1: Loading the Corpus",id:"step-1-loading-the-corpus",level:3},{value:"Step 2: Creating a Label Dictionary",id:"step-2-creating-a-label-dictionary",level:3},{value:"Step 3: Initialize the Model",id:"step-3-initialize-the-model",level:3},{value:"Step 4: Initialize the Trainer",id:"step-4-initialize-the-trainer",level:3},{value:"Step 5: Train",id:"step-5-train",level:3},{value:"Step 6: Predict",id:"step-6-predict",level:3},{value:"Training vs Fine-Tuning",id:"training-vs-fine-tuning",level:2}],c={toc:p},d="wrapper";function u(e){let{components:t,...n}=e;return(0,i.kt)(d,(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"how-model-training-works-in-flair"},"How model training works in Flair"),(0,i.kt)("p",null,"In Flair, all models are trained the same way using the ModelTrainer. This tutorial illustrates\nhow the ModelTrainer works and what decisions you have to make to train good models. "),(0,i.kt)("h2",{id:"example-training-a-part-of-speech-tagger"},"Example: Training a Part-of-Speech Tagger"),(0,i.kt)("p",null,"As example in this chapter, we train a simple part-of-speech tagger for English. To make the example run fast"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"we downsample the training data to 10%"),(0,i.kt)("li",{parentName:"ul"},"we use only simple classic word embeddings (gloVe)")),(0,i.kt)("p",null,"Here is the full training code:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from flair.datasets import UD_ENGLISH\nfrom flair.embeddings import WordEmbeddings\nfrom flair.models import SequenceTagger\nfrom flair.trainers import ModelTrainer\n\n# 1. load the corpus\ncorpus = UD_ENGLISH().downsample(0.1)\nprint(corpus)\n\n# 2. what label do we want to predict?\nlabel_type = 'upos'\n\n# 3. make the label dictionary from the corpus\nlabel_dict = corpus.make_label_dictionary(label_type=label_type)\nprint(label_dict)\n\n# 4. initialize embeddings\nembeddings = WordEmbeddings('glove')\n\n# 5. initialize sequence tagger\nmodel = SequenceTagger(hidden_size=256,\n                        embeddings=embeddings,\n                        tag_dictionary=label_dict,\n                        tag_type=label_type)\n\n# 6. initialize trainer\ntrainer = ModelTrainer(model, corpus)\n\n# 7. start training\ntrainer.train('resources/taggers/example-upos',\n              learning_rate=0.1,\n              mini_batch_size=32,\n              max_epochs=10)\n")),(0,i.kt)("p",null,"This code (1) loads the English universal dependencies dataset as training corpus, (2) create a label dictionary for\nuniversal part-of-speech tags from the corpus, (3) initializes embeddings and (4) runs the trainer for 10 epochs. "),(0,i.kt)("p",null,"Running this script should produce output that looks like this during training: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},'2023-02-27 17:07:38,014 ----------------------------------------------------------------------------------------------------\n2023-02-27 17:07:38,016 Model training base path: "resources/taggers/example-upos"\n2023-02-27 17:07:38,017 ----------------------------------------------------------------------------------------------------\n2023-02-27 17:07:38,020 Device: cuda:0\n2023-02-27 17:07:38,022 ----------------------------------------------------------------------------------------------------\n2023-02-27 17:07:38,023 Embeddings storage mode: cpu\n2023-02-27 17:07:38,025 ----------------------------------------------------------------------------------------------------\n2023-02-27 17:07:39,128 epoch 1 - iter 4/40 - loss 3.28409882 - time (sec): 1.10 - samples/sec: 2611.84 - lr: 0.100000\n2023-02-27 17:07:39,474 epoch 1 - iter 8/40 - loss 3.13510367 - time (sec): 1.45 - samples/sec: 3143.21 - lr: 0.100000\n2023-02-27 17:07:39,910 epoch 1 - iter 12/40 - loss 3.02619775 - time (sec): 1.88 - samples/sec: 3434.39 - lr: 0.100000\n2023-02-27 17:07:40,167 epoch 1 - iter 16/40 - loss 2.95288554 - time (sec): 2.14 - samples/sec: 3783.76 - lr: 0.100000\n2023-02-27 17:07:40,504 epoch 1 - iter 20/40 - loss 2.86820018 - time (sec): 2.48 - samples/sec: 4171.22 - lr: 0.100000\n2023-02-27 17:07:40,843 epoch 1 - iter 24/40 - loss 2.80507526 - time (sec): 2.82 - samples/sec: 4557.72 - lr: 0.100000\n2023-02-27 17:07:41,118 epoch 1 - iter 28/40 - loss 2.74217397 - time (sec): 3.09 - samples/sec: 4878.00 - lr: 0.100000\n2023-02-27 17:07:41,420 epoch 1 - iter 32/40 - loss 2.69161746 - time (sec): 3.39 - samples/sec: 5072.93 - lr: 0.100000\n2023-02-27 17:07:41,705 epoch 1 - iter 36/40 - loss 2.63837577 - time (sec): 3.68 - samples/sec: 5260.02 - lr: 0.100000\n2023-02-27 17:07:41,972 epoch 1 - iter 40/40 - loss 2.58915523 - time (sec): 3.95 - samples/sec: 5394.33 - lr: 0.100000\n2023-02-27 17:07:41,975 ----------------------------------------------------------------------------------------------------\n2023-02-27 17:07:41,977 EPOCH 1 done: loss 2.5892 - lr 0.100000\n2023-02-27 17:07:42,567 DEV : loss 2.009714126586914 - f1-score (micro avg)  0.41\n2023-02-27 17:07:42,579 BAD EPOCHS (no improvement): 0\n')),(0,i.kt)("p",null,"The output monitors the loss over the epochs. At the end of each epoch, the development score is computed and printed."),(0,i.kt)("p",null,"And a ",(0,i.kt)("strong",{parentName:"p"},"final evaluation report")," gets printed in the end: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"Results:\n- F-score (micro) 0.7732\n- F-score (macro) 0.6329\n- Accuracy 0.7732\n\nBy class:\n              precision    recall  f1-score   support\n\n        NOUN     0.7199    0.7199    0.7199       407\n       PUNCT     0.9263    0.9843    0.9544       319\n        VERB     0.7521    0.6938    0.7218       258\n        PRON     0.7782    0.9300    0.8474       200\n         ADP     0.8559    0.9515    0.9011       206\n       PROPN     0.6585    0.6398    0.6490       211\n         ADJ     0.5654    0.6914    0.6221       175\n         DET     0.9572    0.8995    0.9275       199\n         AUX     0.8609    0.8784    0.8696       148\n         ADV     0.5052    0.5000    0.5026        98\n       CCONJ     0.9833    0.9077    0.9440        65\n         NUM     0.5435    0.3289    0.4098        76\n        PART     0.9091    0.7143    0.8000        56\n       SCONJ     0.7083    0.5667    0.6296        30\n         SYM     0.3333    0.2143    0.2609        14\n           X     0.0000    0.0000    0.0000        15\n        INTJ     0.0000    0.0000    0.0000        14\n\n    accuracy                         0.7732      2491\n   macro avg     0.6504    0.6247    0.6329      2491\nweighted avg     0.7635    0.7732    0.7655      2491\n")),(0,i.kt)("p",null,"This report gives us a breakdown of the precision, recall and F1 score of all classes, as well as overall. "),(0,i.kt)("p",null,"Congrats, you just trained your first model!"),(0,i.kt)("h2",{id:"step-by-step-walkthrough"},"Step-By-Step Walkthrough"),(0,i.kt)("p",null,"The above code showed you how to train a PoS tagger. "),(0,i.kt)("p",null,"Now let's look at each of the main steps in the above script:  "),(0,i.kt)("h3",{id:"step-1-loading-the-corpus"},"Step 1: Loading the Corpus"),(0,i.kt)("p",null,"In this example, we use the English Universal Dependencies Dataset to train on. It contains many sentences fully annotated\nwith both universal and language-specific part-of-speech tags. Running these lines will load and print the corpus: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"corpus = UD_ENGLISH().downsample(0.1)\nprint(corpus)\n")),(0,i.kt)("p",null,"which should print:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"Corpus: 1254 train + 200 dev + 208 test sentences\n")),(0,i.kt)("p",null,"Showing us that our downsampled training data has three splits: a training split of 1254 sentences, a dev split of 200 sentences, and a test split of 208 sentences."),(0,i.kt)("p",null,"The ",(0,i.kt)("strong",{parentName:"p"},"Corpus")," is a very handy concept in Flair, with lots of helper functions. To learn all that it can do, check out ..."),(0,i.kt)("h3",{id:"step-2-creating-a-label-dictionary"},"Step 2: Creating a Label Dictionary"),(0,i.kt)("p",null,"Our model needs to predict a set of labels. To determine the label set, run make_label_dictionary on the corpus\nand pass the label type you want to predict. In this example, we pass upos since we want to predict universal\npart-of-speech tags. "),(0,i.kt)("p",null,"Running these lines will compute and print the label dictionary from the corpus: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"label_dict = corpus.make_label_dictionary(label_type=label_type)\nprint(label_dict)\n")),(0,i.kt)("p",null,"which should print:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"Dictionary with 18 tags: <unk>, NOUN, PUNCT, VERB, PRON, ADP, DET, AUX, ADJ, PROPN, ADV, CCONJ, PART, SCONJ, NUM, X, SYM, INTJ\n")),(0,i.kt)("p",null,"Showing us that our label dictionary has 18 PoS tags, including one generic tag for all unknown labels."),(0,i.kt)("h3",{id:"step-3-initialize-the-model"},"Step 3: Initialize the Model"),(0,i.kt)("p",null,"Depending on what you want to do, you need to initialize the appropriate model type. For sequence labeling\n(NER, part-of-speech tagging) you need the ",(0,i.kt)("inlineCode",{parentName:"p"},"SequenceLabeler"),". For text classification you need the ",(0,i.kt)("inlineCode",{parentName:"p"},"TextClassifier."),"\nFor each model type, we are creating dedicated tutorials to better explain what they do."),(0,i.kt)("p",null,"For this example, we use the SequenceLabeler since we do part-of-speech tagging: "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# 5. initialize sequence tagger\nmodel = SequenceTagger(hidden_size=256,\n                       embeddings=embeddings,\n                       tag_dictionary=label_dict,\n                       tag_type=label_type)\n")),(0,i.kt)("p",null,"Printing it will give you the PyTorch model that is initialized. "),(0,i.kt)("h3",{id:"step-4-initialize-the-trainer"},"Step 4: Initialize the Trainer"),(0,i.kt)("p",null,"The ModelTrainer is initialized simply by passing the model and the corpus because that is all it needs."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"trainer = ModelTrainer(model, corpus)\n")),(0,i.kt)("h3",{id:"step-5-train"},"Step 5: Train"),(0,i.kt)("p",null,"Once the trainer is initialized, you can call ",(0,i.kt)("inlineCode",{parentName:"p"},"train")," to launch a standard training run. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"trainer.train('resources/taggers/example-upos',\n              learning_rate=0.1,\n              mini_batch_size=32,\n              max_epochs=10)\n")),(0,i.kt)("p",null,'This will launch a "standard training run" with SGD as optimizer. By default, the learning rate is annealed against the development score: if\nfo 3 epochs there is no improvement on the dev split, the learning rate is halved. If this happens too often, the learning rate will fall below\na minimal threshold and training stops early. '),(0,i.kt)("p",null,"The max_epochs parameter is set to a small number in this script to make it run fast, but normally you should use a much higher value (150 or 200). "),(0,i.kt)("h3",{id:"step-6-predict"},"Step 6: Predict"),(0,i.kt)("p",null,"Once the model is trained you can use it to predict tags for new sentences. Just call the ",(0,i.kt)("inlineCode",{parentName:"p"},"predict")," method of the model."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# load the model you trained\nmodel = SequenceTagger.load('resources/taggers/example-upos/final-model.pt')\n\n# create example sentence\nsentence = Sentence('I love Berlin')\n\n# predict tags and print\nmodel.predict(sentence)\n\nprint(sentence.to_tagged_string())\n")),(0,i.kt)("p",null,"If the model works well, it will correctly tag 'love' as a verb in this example."),(0,i.kt)("h2",{id:"training-vs-fine-tuning"},"Training vs Fine-Tuning"))}u.isMDXComponent=!0}}]);