"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8585],{3905:(e,n,t)=>{t.d(n,{Zo:()=>m,kt:()=>g});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function s(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?s(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):s(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function o(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},s=Object.keys(e);for(r=0;r<s.length;r++)t=s[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(r=0;r<s.length;r++)t=s[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var d=r.createContext({}),l=function(e){var n=r.useContext(d),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},m=function(e){var n=l(e.components);return r.createElement(d.Provider,{value:n},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},c=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,s=e.originalType,d=e.parentName,m=o(e,["components","mdxType","originalType","parentName"]),u=l(t),c=a,g=u["".concat(d,".").concat(c)]||u[c]||p[c]||s;return t?r.createElement(g,i(i({ref:n},m),{},{components:t})):r.createElement(g,i({ref:n},m))}));function g(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var s=t.length,i=new Array(s);i[0]=c;var o={};for(var d in n)hasOwnProperty.call(n,d)&&(o[d]=n[d]);o.originalType=e,o[u]="string"==typeof e?e:a,i[1]=o;for(var l=2;l<s;l++)i[l]=t[l];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}c.displayName="MDXCreateElement"},4187:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>l});var r=t(7462),a=(t(7294),t(3905));const s={sidebar_position:2,description:"The most important embeddings are based on transformers"},i="Transformer embeddings",o={unversionedId:"tutorial-embeddings/transformer-embeddings",id:"tutorial-embeddings/transformer-embeddings",title:"Transformer embeddings",description:"The most important embeddings are based on transformers",source:"@site/docs/tutorial-embeddings/transformer-embeddings.md",sourceDirName:"tutorial-embeddings",slug:"/tutorial-embeddings/transformer-embeddings",permalink:"/docs/tutorial-embeddings/transformer-embeddings",draft:!1,editUrl:"https://github.com/flairNLP/flairnlp.github.io/edit/main/docs/tutorial-embeddings/transformer-embeddings.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,description:"The most important embeddings are based on transformers"},sidebar:"tutorialSidebar",previous:{title:"Embeddings",permalink:"/docs/tutorial-embeddings/embeddings"},next:{title:"Flair embeddings",permalink:"/docs/tutorial-embeddings/flair-embeddings"}},d={},l=[{value:"Embeddings words",id:"embeddings-words",level:2},{value:"Embeddings sentences",id:"embeddings-sentences",level:2},{value:"Arguments",id:"arguments",level:2},{value:"Layers",id:"layers",level:3},{value:"Pooling operation",id:"pooling-operation",level:3},{value:"Layer mean",id:"layer-mean",level:3},{value:"Fine-tuneable or not",id:"fine-tuneable-or-not",level:3},{value:"Models",id:"models",level:3}],m={toc:l},u="wrapper";function p(e){let{components:n,...t}=e;return(0,a.kt)(u,(0,r.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"transformer-embeddings"},"Transformer embeddings"),(0,a.kt)("p",null,"Flair supports various Transformer-based architectures like BERT or XLNet from ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/huggingface"},"HuggingFace"),",\nwith two classes ",(0,a.kt)("inlineCode",{parentName:"p"},"TransformerWordEmbeddings")," (to embed words) and ",(0,a.kt)("inlineCode",{parentName:"p"},"TransformerDocumentEmbeddings")," (to embed documents)."),(0,a.kt)("h2",{id:"embeddings-words"},"Embeddings words"),(0,a.kt)("p",null,"For instance, to load a standard BERT transformer model, do:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from flair.embeddings import TransformerWordEmbeddings\n\n# init embedding\nembedding = TransformerWordEmbeddings('bert-base-uncased')\n\n# create a sentence\nsentence = Sentence('The grass is green .')\n\n# embed words in sentence\nembedding.embed(sentence)\n")),(0,a.kt)("p",null,"If instead you want to use RoBERTa, do:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from flair.embeddings import TransformerWordEmbeddings\n\n# init embedding\nembedding = TransformerWordEmbeddings('roberta-base')\n\n# create a sentence\nsentence = Sentence('The grass is green .')\n\n# embed words in sentence\nembedding.embed(sentence)\n")),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://huggingface.co/transformers/pretrained_models.html"},"Here")," is a full list of all models (BERT, RoBERTa, XLM, XLNet etc.). You can use any of these models with this class."),(0,a.kt)("h2",{id:"embeddings-sentences"},"Embeddings sentences"),(0,a.kt)("p",null,"To embed a whole sentence as one (instead of each word in the sentence), simply use the TransformerDocumentEmbeddings\ninstead:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from flair.embeddings import TransformerDocumentEmbeddings\n\n# init embedding\nembedding = TransformerDocumentEmbeddings('roberta-base')\n\n# create a sentence\nsentence = Sentence('The grass is green .')\n\n# embed words in sentence\nembedding.embed(sentence)\n")),(0,a.kt)("h2",{id:"arguments"},"Arguments"),(0,a.kt)("p",null,"There are several options that you can set when you init the TransformerWordEmbeddings\nand TransformerDocumentEmbeddings classes:"),(0,a.kt)("table",null,(0,a.kt)("thead",{parentName:"table"},(0,a.kt)("tr",{parentName:"thead"},(0,a.kt)("th",{parentName:"tr",align:null},"Argument"),(0,a.kt)("th",{parentName:"tr",align:null},"Default"),(0,a.kt)("th",{parentName:"tr",align:null},"Description"))),(0,a.kt)("tbody",{parentName:"table"},(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"model")),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"bert-base-uncased")),(0,a.kt)("td",{parentName:"tr",align:null},"The string identifier of the transformer model you want to use (see above)")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"layers")),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"all")),(0,a.kt)("td",{parentName:"tr",align:null},"Defines the layers of the Transformer-based model that produce the embedding")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"subtoken_pooling")),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"first")),(0,a.kt)("td",{parentName:"tr",align:null},"See ",(0,a.kt)("a",{parentName:"td",href:"#Pooling-operation"},"Pooling operation section"),".")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"layer_mean")),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"True")),(0,a.kt)("td",{parentName:"tr",align:null},"See ",(0,a.kt)("a",{parentName:"td",href:"#Layer-mean"},"Layer mean section"),".")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"fine_tune")),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"False")),(0,a.kt)("td",{parentName:"tr",align:null},"Whether or not embeddings are fine-tuneable.")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"allow_long_sentences")),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"True")),(0,a.kt)("td",{parentName:"tr",align:null},"Whether or not texts longer than maximal sequence length are supported.")),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"use_context")),(0,a.kt)("td",{parentName:"tr",align:null},(0,a.kt)("inlineCode",{parentName:"td"},"False")),(0,a.kt)("td",{parentName:"tr",align:null},"Set to True to include context outside of sentences. This can greatly increase accuracy on some tasks, but slows down embedding generation")))),(0,a.kt)("h3",{id:"layers"},"Layers"),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"layers")," argument controls which transformer layers are used for the embedding. If you set this value to '-1,-2,-3,-4', the top 4 layers are used to make an embedding. If you set it to '-1', only the last layer is used. If you set it to \"all\", then all layers are used."),(0,a.kt)("p",null,"This affects the length of an embedding, since layers are just concatenated."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"from flair.data import Sentence\nfrom flair.embeddings import TransformerWordEmbeddings\n\nsentence = Sentence('The grass is green.')\n\n# use only last layers\nembeddings = TransformerWordEmbeddings('bert-base-uncased', layers='-1', layer_mean=False)\nembeddings.embed(sentence)\nprint(sentence[0].embedding.size())\n\nsentence.clear_embeddings()\n\n# use last two layers\nembeddings = TransformerWordEmbeddings('bert-base-uncased', layers='-1,-2', layer_mean=False)\nembeddings.embed(sentence)\nprint(sentence[0].embedding.size())\n\nsentence.clear_embeddings()\n\n# use ALL layers\nembeddings = TransformerWordEmbeddings('bert-base-uncased', layers='all', layer_mean=False)\nembeddings.embed(sentence)\nprint(sentence[0].embedding.size())\n")),(0,a.kt)("p",null,"This should print:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"torch.Size([768])\ntorch.Size([1536])\ntorch.Size([9984])\n")),(0,a.kt)("p",null,"I.e. the size of the embedding increases the mode layers we use (but ONLY if layer_mean is set to False, otherwise the length is always the same)."),(0,a.kt)("h3",{id:"pooling-operation"},"Pooling operation"),(0,a.kt)("p",null,"Most of the Transformer-based models (except Transformer-XL) use subword tokenization. E.g. the following\ntoken ",(0,a.kt)("inlineCode",{parentName:"p"},"puppeteer")," could be tokenized into the subwords: ",(0,a.kt)("inlineCode",{parentName:"p"},"pupp"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"##ete")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"##er"),"."),(0,a.kt)("p",null,"We implement different pooling operations for these subwords to generate the final token representation:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"first"),": only the embedding of the first subword is used"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"last"),": only the embedding of the last subword is used"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"first_last"),": embeddings of the first and last subwords are concatenated and used"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"mean"),": a ",(0,a.kt)("inlineCode",{parentName:"li"},"torch.mean")," over all subword embeddings is calculated and used")),(0,a.kt)("p",null,"You can choose which one to use by passing this in the constructor:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# use first and last subtoken for each word\nembeddings = TransformerWordEmbeddings('bert-base-uncased', subtoken_pooling='first_last')\nembeddings.embed(sentence)\nprint(sentence[0].embedding.size())\n")),(0,a.kt)("h3",{id:"layer-mean"},"Layer mean"),(0,a.kt)("p",null,"The Transformer-based models have a certain number of layers. By default, all layers you select are\nconcatenated as explained above. Alternatively, you can set layer_mean=True to do a mean over all\nselected layers. The resulting vector will then always have the same dimensionality as a single layer:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'from flair.embeddings import TransformerWordEmbeddings\n\n# init embedding\nembedding = TransformerWordEmbeddings("roberta-base", layers="all", layer_mean=True)\n\n# create a sentence\nsentence = Sentence("The Oktoberfest is the world\'s largest Volksfest .")\n\n# embed words in sentence\nembedding.embed(sentence)\n')),(0,a.kt)("h3",{id:"fine-tuneable-or-not"},"Fine-tuneable or not"),(0,a.kt)("p",null,"In some setups, you may wish to fine-tune the transformer embeddings. In this case, set ",(0,a.kt)("inlineCode",{parentName:"p"},"fine_tune=True")," in the init method.\nWhen fine-tuning, you should also only use the topmost layer, so best set ",(0,a.kt)("inlineCode",{parentName:"p"},"layers='-1'"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# use first and last subtoken for each word\nembeddings = TransformerWordEmbeddings('bert-base-uncased', fine_tune=True, layers='-1')\nembeddings.embed(sentence)\nprint(sentence[0].embedding)\n")),(0,a.kt)("p",null,"This will print a tensor that now has a gradient function and can be fine-tuned if you use it in a training routine."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"tensor([-0.0323, -0.3904, -1.1946,  ...,  0.1305, -0.1365, -0.4323],\n       device='cuda:0', grad_fn=<CatBackward>)\n")),(0,a.kt)("h3",{id:"models"},"Models"),(0,a.kt)("p",null,"Please have a look at the awesome Hugging Face ",(0,a.kt)("a",{parentName:"p",href:"https://huggingface.co/transformers/v2.3.0/pretrained_models.html"},"documentation"),"\nfor all supported pretrained models!"))}p.isMDXComponent=!0}}]);